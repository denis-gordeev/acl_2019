@inproceedings{lazaridou-parallel,
address = {Stroudsburg, PA, USA},
author = {Lazaridou, Angeliki and Dinu, Georgiana and Baroni, Marco},
booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
doi = {10.3115/v1/P15-1027},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lazaridou, Dinu, Baroni - 2015 - Hubness and Pollution Delving into Cross-Space Mapping for Zero-Shot Learning.pdf:pdf},
pages = {270--280},
publisher = {Association for Computational Linguistics},
title = {{Hubness and Pollution: Delving into Cross-Space Mapping for Zero-Shot Learning}},
url = {http://aclweb.org/anthology/P15-1027},
volume = {1},
year = {2015}
}
@inproceedings{NEALE18.1030,
address = {Miyazaki, Japan},
author = {Neale, Steven},
booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)},
editor = {Chair), Nicoletta Calzolari (Conference and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Hasida, Koiti and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, H{\'{e}}l{\`{e}}ne and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios and Tokunaga, Takenobu},
isbn = {979-10-95546-00-9},
publisher = {European Language Resources Association (ELRA)},
title = {{A Survey on Automatically-Constructed WordNets and their Evaluation: Lexical and Word Embedding-based Approaches}},
year = {2018}
}
@article{mikolov-parallel,
abstract = {Dictionaries and phrase tables are the basis of modern statistical machine translation systems. This paper develops a method that can automate the process of generating and extending dictionaries and phrase tables. Our method can translate missing word and phrase entries by learning language structures based on large monolingual data and mapping between languages from small bilingual data. It uses distributed representation of words and learns a linear mapping between vector spaces of languages. Despite its simplicity, our method is surprisingly effective: we can achieve almost 90{\%} precision@5 for translation of words between English and Spanish. This method makes little assumption about the languages, so it can be used to extend and refine dictionaries and translation tables for any language pairs.},
archivePrefix = {arXiv},
arxivId = {1309.4168},
author = {Mikolov, Tomas and Le, Quoc V. and Sutskever, Ilya},
eprint = {1309.4168},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov, Le, Sutskever - 2013 - Exploiting Similarities among Languages for Machine Translation.pdf:pdf},
month = {sep},
title = {{Exploiting Similarities among Languages for Machine Translation}},
url = {http://arxiv.org/abs/1309.4168},
year = {2013}
}
@inproceedings{Khodak2017,
address = {Stroudsburg, PA, USA},
author = {Khodak, Mikhail and Risteski, Andrej and Fellbaum, Christiane and Arora, Sanjeev},
booktitle = {Proceedings of the 1st Workshop on Sense, Concept and Entity
          Representations and their Applications},
doi = {10.18653/v1/W17-1902},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Khodak et al. - 2017 - Automated WordNet Construction Using Word Embeddings.pdf:pdf},
pages = {12--23},
publisher = {Association for Computational Linguistics},
title = {{Automated WordNet Construction Using Word Embeddings}},
url = {http://aclweb.org/anthology/W17-1902},
year = {2017}
}
@inproceedings{faria,
author = {Faria, Daniel and Pesquita, Catia and Santos, Emanuel and Palmonari, Matteo and Cruz, Isabel F. and Couto, Francisco M.},
doi = {10.1007/978-3-642-41030-7_38},
pages = {527--541},
title = {{The AgreementMakerLight Ontology Matching System}},
url = {http://link.springer.com/10.1007/978-3-642-41030-7{\_}38},
year = {2013}
}
@article{wordnet,
author = {Miller, George A.},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/ACM, 1995 - Unknown - WordNet a lexical database for English.pdf:pdf},
journal = {Communications of the ACM},
number = {11},
pages = {39--41},
title = {{WordNet: a lexical database for English}},
url = {https://dl.acm.org/citation.cfm?id=219748},
volume = {38},
year = {1995}
}
@article{hedging-bets,
author = {Deng, J and Krause, J and on {\ldots}, AC Berg - 2012 IEEE Conference and 2012, Undefined},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Deng et al. - Unknown - Hedging your bets Optimizing accuracy-specificity trade-offs in large scale visual recognition.pdf:pdf},
journal = {ieeexplore.ieee.org},
title = {{Hedging your bets: Optimizing accuracy-specificity trade-offs in large scale visual recognition}},
url = {https://ieeexplore.ieee.org/abstract/document/6248086/}
}
@article{glomo,
abstract = {Modern deep transfer learning approaches have mainly focused on learning generic feature vectors from one task that are transferable to other tasks, such as word embeddings in language and pretrained convolutional features in vision. However, these approaches usually transfer unary features and largely ignore more structured graphical representations. This work explores the possibility of learning generic latent relational graphs that capture dependencies between pairs of data units (e.g., words or pixels) from large-scale unlabeled data and transferring the graphs to downstream tasks. Our proposed transfer learning framework improves performance on various tasks including question answering, natural language inference, sentiment analysis, and image classification. We also show that the learned graphs are generic enough to be transferred to different embeddings on which the graphs have not been trained (including GloVe embeddings, ELMo embeddings, and task-specific RNN hidden unit), or embedding-free units such as image pixels.},
archivePrefix = {arXiv},
arxivId = {1806.05662},
author = {Yang, Zhilin and Zhao, Jake and Dhingra, Bhuwan and He, Kaiming and Cohen, William W. and Salakhutdinov, Ruslan and LeCun, Yann},
eprint = {1806.05662},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang et al. - 2018 - GLoMo Unsupervisedly Learned Relational Graphs as Transferable Representations.pdf:pdf},
month = {jun},
title = {{GLoMo: Unsupervisedly Learned Relational Graphs as Transferable Representations}},
url = {http://arxiv.org/abs/1806.05662},
year = {2018}
}
@article{ontology-sota,
author = {Shvaiko, P and on knowledge And, J Euzenat - IEEE Transactions and 2013, Undefined},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shvaiko, and, 2013 - Unknown - Ontology matching state of the art and future challenges.pdf:pdf},
journal = {ieeexplore.ieee.org},
title = {{Ontology matching: state of the art and future challenges}},
url = {https://ieeexplore.ieee.org/abstract/document/6104044/}
}
@article{tax2vec,
abstract = {The use of background knowledge remains largely unexploited in many text classification tasks. In this work, we explore word taxonomies as means for constructing new semantic features, which may improve the performance and robustness of the learned classifiers. We propose tax2vec, a parallel algorithm for constructing taxonomy based features, and demonstrate its use on six short-text classification problems, including gender, age and personality type prediction, drug effectiveness and side effect prediction, and news topic prediction. The experimental results indicate that the interpretable features constructed using tax2vec can notably improve the performance of classifiers; the constructed features, in combination with fast, linear classifiers tested against strong baselines, such as hierarchical attention neural networks, achieved comparable or better classification results on short documents. Further, tax2vec can also serve for extraction of corpus-specific keywords. Finally, we investigated the semantic space of potential features where we observe a similarity with the well known Zipf's law.},
archivePrefix = {arXiv},
arxivId = {1902.00438},
author = {{\v{S}}krlj, Bla{\v{z}} and Martinc, Matej and Kralj, Jan and Lavra{\v{c}}, Nada and Pollak, Senja},
eprint = {1902.00438},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/{\v{S}}krlj et al. - 2019 - tax2vec Constructing Interpretable Features from Taxonomies for Short Text Classification.pdf:pdf},
month = {feb},
title = {{tax2vec: Constructing Interpretable Features from Taxonomies for Short Text Classification}},
url = {http://arxiv.org/abs/1902.00438},
year = {2019}
}
@inproceedings{Swoboda2016,
address = {New York, New York, USA},
author = {Swoboda, Tobias and Hemmje, Matthias and Dascalu, Mihai and Trausan-Matu, Stefan},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering - DocEng '16},
doi = {10.1145/2960811.2967151},
isbn = {9781450344388},
keywords = {automated semantic integration,ontology alignment,taxonomy integration,word2vec},
pages = {131--134},
publisher = {ACM Press},
title = {{Combining Taxonomies using Word2vec}},
url = {http://dl.acm.org/citation.cfm?doid=2960811.2967151},
year = {2016}
}
@inproceedings{maslova-potapov,
author = {Maslova, Natalia and Potapov, Vsevolod},
booktitle = {Lecture Notes in Computer Science},
doi = {10.1007/978-3-319-66429-3_54},
isbn = {9783319664286},
issn = {16113349},
keywords = {Automated sentiment analysis,Deprivation,Russian,Social network discourse,Supervised learning,Word embeddings},
pages = {546--554},
title = {{Neural network doc2vec in automated sentiment analysis for short informal texts}},
url = {http://link.springer.com/10.1007/978-3-319-66429-3{\_}54},
volume = {10458 LNAI},
year = {2017}
}
@article{google-translate-rare,
abstract = {Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units ("wordpieces") for both input and output. This method provides a good balance between the flexibility of "character"-delimited models and the efficiency of "word"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60{\%} compared to Google's phrase-based production system.},
archivePrefix = {arXiv},
arxivId = {1609.08144},
author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V.},
eprint = {1609.08144},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu et al. - 2016 - Google's Neural Machine Translation System Bridging the Gap between Human and Machine Translation.pdf:pdf},
month = {sep},
title = {{Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation}},
url = {http://arxiv.org/abs/1609.08144},
year = {2016}
}
@article{bilda,
abstract = {In this paper, we study different applications of cross-language latent topic models trained on comparable corpora. The first focus lies on the task of cross-language information retrieval (CLIR). The Bilingual Latent Dirichlet allocation model (BiLDA) allows us to create an interlingual, language-independent representation of both queries and documents. We construct several BiLDA-based document models for CLIR, where no additional translation resources are used. The second focus lies on the methods for extracting translation candidates and semantically related words using only per-topic word distributions of the cross-language latent topic model. As the main contribution, we combine the two former steps, blending the evidences from the per-document topic dis- tributions and the per-topic word distributions of the topic model with the knowledge from the extracted lexicon. We design and evaluate the novel evidence-rich statistical model for CLIR, and prove that such a model, which combines various (only internal) evidences, obtains the best scores for experiments performed on the standard test collections of the CLEF 2001–2003 campaigns. We confirm these findings in an alternative evaluation, where we automatically generate queries and perform the known-item search on a test subset of Wikipedia articles. The main importance of this work lies in the fact that we train translation resources from comparable document-aligned corpora and provide novel CLIR statistical models that exhaustively exploit as many cross-lingual clues as possible in the quest for better CLIR results, without use of any additional external resources such as parallel corpora or machine-readable dictionaries.},
author = {Vuli{\'{c}}, Ivan and de Smet, Wim and Moens, Marie Francine},
doi = {10.1007/s10791-012-9200-5},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vuli{\'{c}}, De Smet, Moens - 2013 - Cross-language information retrieval models based on latent topic models trained with document-aligned c.pdf:pdf},
isbn = {1079101292},
issn = {13864564},
journal = {Information Retrieval},
keywords = {Cross-language information retrieval,Evidence-rich retrieval models,Probabilistic latent topic models,Unsupervised cross-language lexicon extraction},
month = {jun},
number = {3},
pages = {331--368},
publisher = {Springer Netherlands},
title = {{Cross-language information retrieval models based on latent topic models trained with document-aligned comparable corpora}},
url = {http://link.springer.com/10.1007/s10791-012-9200-5},
volume = {16},
year = {2013}
}
@inproceedings{short-lda,
abstract = {Nowadays, short texts are very prevalent in various web applications, such as microblogs, instant messages. The severe sparsity of short texts hinders existing topic models to learn reliable topics. In this paper, we propose a novel way to tackle this problem. The key idea is to learn topics by exploring term correlation data, rather than the high-dimensional and sparse term occurrence information in documents. Such term correlation data is less sparse and more stable with the increase of the collection size, and can well capture the necessary information for topic learning. To obtain reliable topics from term correlation data, we first introduce a novel way to compute term correlation in short texts by representing each term with its co-occurred terms. Then we formulated the topic learning problem as symmetric non-negative matrix factorization on the term correlation matrix. After learning the topics, we can easily infer the topics of documents. Experimental results on three data sets show that our method provides substantially better performance than the baseline methods.},
address = {Philadelphia, PA},
author = {Yan, Xiaohui and Guo, Jiafeng and Liu, Shenghua and Cheng, Xueqi and Wang, Yanfeng},
booktitle = {Proceedings of the 2013 SIAM International Conference on Data Mining},
doi = {10.1137/1.9781611972832.83},
isbn = {9781611972627},
issn = {9781611972627},
month = {may},
pages = {749--757},
publisher = {Society for Industrial and Applied Mathematics},
title = {{Learning Topics in Short Texts by Non-negative Matrix Factorization on Term Correlation Matrix}},
url = {http://epubs.siam.org/doi/abs/10.1137/1.9781611972832.83},
year = {2013}
}
@misc{unsd,
title = {{UNSD — National Classifications}},
url = {https://unstats.un.org/unsd/classifications/nationalclassifications/},
urldate = {2018-09-11}
}
@book{dupe-detect,
address = {Berlin Heidelberg},
author = {Christen, Peter},
doi = {10.1007/978-3-642-31164-2},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Christen - 2012 - Data matching concepts and techniques for record linkage, entity resolution, and duplicate detection.pdf:pdf},
isbn = {978-3-642-31163-5},
pages = {272},
publisher = {Springer-Verlag},
title = {{Data Matching: Concepts and Techniques for Record Linkage, Entity Resolution, and Duplicate Detection}},
year = {2012}
}
@article{jawanpuria,
abstract = {We propose a novel geometric approach for learning bilingual mappings given monolingual embeddings and a bilingual dictionary. Our approach decouples learning the transformation from the source language to the target language into (a) learning rotations for language-specific embeddings to align them to a common space, and (b) learning a similarity metric in the common space to model similarities between the embeddings. We model the bilingual mapping problem as an optimization problem on smooth Riemannian manifolds. We show that our approach outperforms previous approaches on the bilingual lexicon induction and cross-lingual word similarity tasks. We also generalize our framework to represent multiple languages in a common latent space. In particular, the latent space representations for several languages are learned jointly, given bilingual dictionaries for multiple language pairs. We illustrate the effectiveness of joint learning for multiple languages in zero-shot word translation setting.},
archivePrefix = {arXiv},
arxivId = {1808.08773},
author = {Jawanpuria, Pratik and Balgovind, Arjun and Kunchukuttan, Anoop and Mishra, Bamdev},
eprint = {1808.08773},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jawanpuria et al. - 2018 - Learning Multilingual Word Embeddings in Latent Metric Space A Geometric Approach.pdf:pdf},
month = {aug},
title = {{Learning Multilingual Word Embeddings in Latent Metric Space: A Geometric Approach}},
url = {http://arxiv.org/abs/1808.08773},
year = {2018}
}
@article{fasttext,
abstract = {This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore{\~{}}CPU, and classify half a million sentences among{\~{}}312K classes in less than a minute.},
archivePrefix = {arXiv},
arxivId = {1607.01759},
author = {Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},
eprint = {1607.01759},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Joulin et al. - 2016 - Bag of Tricks for Efficient Text Classification.pdf:pdf},
month = {jul},
title = {{Bag of Tricks for Efficient Text Classification}},
url = {http://arxiv.org/abs/1607.01759},
year = {2016}
}
@inproceedings{dinu,
abstract = {The zero-shot paradigm exploits vector-based word representations extracted from text corpora with unsupervised methods to learn general mapping functions from other feature spaces onto word space, where the words associated to the nearest neighbours of the mapped vectors are used as their linguistic labels. We show that the neighbourhoods of the mapped elements are strongly polluted by hubs, vectors that tend to be near a high proportion of items, pushing their correct labels down the neighbour list. After illustrating the problem empirically, we propose a simple method to correct it by taking the proximity distribution of potential neighbours across many mapped vectors into account. We show that this correction leads to consistent improvements in realistic zero-shot experiments in the cross-lingual, image labeling and image retrieval domains.},
archivePrefix = {arXiv},
arxivId = {1412.6568},
author = {Dinu, Georgiana and Lazaridou, Angeliki and Baroni, Marco},
booktitle = {In Proceedings of the 3rd In- ternational Conference on Learning Representations (ICLR2015), workshop track},
eprint = {1412.6568},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dinu, Lazaridou, Baroni - 2014 - Improving zero-shot learning by mitigating the hubness problem.pdf:pdf},
month = {dec},
title = {{Improving zero-shot learning by mitigating the hubness problem}},
url = {http://arxiv.org/abs/1412.6568},
year = {2015}
}
@misc{data-gov,
title = {{Datasets - Data.gov}},
url = {https://catalog.data.gov/dataset},
urldate = {2018-09-04}
}
@misc{gos-zakupki,
title = {{The official site of the Unified Information System in the field of procurement [Oficial'nyj sajt Edinoj informacionnoj sistemy v sfere zakupok]}},
url = {http://www.zakupki.gov.ru/},
urldate = {2018-09-04}
}
@article{levy-goldberg-2015,
abstract = {Recent trends suggest that neural-network-inspired word embedding models outperform traditional count-based distri-butional models on word similarity and analogy detection tasks. We reveal that much of the performance gains of word embeddings are due to certain system design choices and hyperparameter op-timizations, rather than the embedding algorithms themselves. Furthermore, we show that these modifications can be transferred to traditional distributional models, yielding similar gains. In contrast to prior reports, we observe mostly local or insignificant performance differences between the methods, with no global advantage to any single approach over the others.},
archivePrefix = {arXiv},
arxivId = {1103.0398},
author = {Levy, Omer and Goldberg, Yoav and Dagan, Ido},
doi = {10.1186/1472-6947-15-S2-S2},
eprint = {1103.0398},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Levy et al. - Unknown - Improving distributional similarity with lessons learned from word embeddings.pdf:pdf},
issn = {2307-387X},
journal = {Transactions of the Association for Computational Linguistics},
pages = {211--225},
pmid = {26099735},
title = {{Improving Distributional Similarity with Lessons Learned from Word Embeddings}},
url = {https://www.transacl.org/ojs/index.php/tacl/article/view/570},
volume = {3},
year = {2015}
}
@article{bengio,
author = {Bengio, Y and Ducharme, R and Vincent, P},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bengio et al. - Unknown - A neural probabilistic language model.pdf:pdf},
issn = {ISSN 1533-7928},
journal = {Journal of Machine Learning Research},
keywords = {artificial neural networks,curse of dimensionality,distributed representation,statistical language modeling},
pages = {1137--1155},
title = {{A neural probabilistic language model}},
url = {http://www.jmlr.org/papers/v3/bengio03a.html},
volume = {3},
year = {2003}
}
@article{ruder-muse-limitations,
abstract = {Unsupervised machine translation---i.e., not assuming any cross-lingual supervision signal, whether a dictionary, translations, or comparable corpora---seems impossible, but nevertheless, Lample et al. (2018) recently proposed a fully unsupervised machine translation (MT) model. The model relies heavily on an adversarial, unsupervised alignment of word embedding spaces for bilingual dictionary induction (Conneau et al., 2018), which we examine here. Our results identify the limitations of current unsupervised MT: unsupervised bilingual dictionary induction performs much worse on morphologically rich languages that are not dependent marking, when monolingual corpora from different domains or different embedding algorithms are used. We show that a simple trick, exploiting a weak supervision signal from identical words, enables more robust induction, and establish a near-perfect correlation between unsupervised bilingual dictionary induction performance and a previously unexplored graph similarity metric.},
archivePrefix = {arXiv},
arxivId = {1805.03620},
author = {S{\o}gaard, Anders and Ruder, Sebastian and Vuli{\'{c}}, Ivan},
eprint = {1805.03620},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/S{\o}gaard, Ruder, Vuli{\'{c}} - 2018 - On the Limitations of Unsupervised Bilingual Dictionary Induction.pdf:pdf},
month = {may},
title = {{On the Limitations of Unsupervised Bilingual Dictionary Induction}},
url = {http://arxiv.org/abs/1805.03620},
year = {2018}
}
@article{vecmap,
abstract = {Recent work has managed to learn cross-lingual word embeddings without parallel data by mapping monolingual embeddings to a shared space through adversarial training. However, their evaluation has focused on favorable conditions, using comparable corpora or closely-related languages, and we show that they often fail in more realistic scenarios. This work proposes an alternative approach based on a fully unsupervised initialization that explicitly exploits the structural similarity of the embeddings, and a robust self-learning algorithm that iteratively improves this solution. Our method succeeds in all tested scenarios and obtains the best published results in standard datasets, even surpassing previous supervised systems. Our implementation is released as an open source project at https://github.com/artetxem/vecmap},
archivePrefix = {arXiv},
arxivId = {1805.06297},
author = {Artetxe, Mikel and Labaka, Gorka and Agirre, Eneko},
eprint = {1805.06297},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Artetxe, Labaka, Agirre - 2018 - A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings.pdf:pdf},
month = {may},
title = {{A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings}},
url = {http://arxiv.org/abs/1805.06297},
year = {2018}
}
@misc{wiki-okpd,
title = {{All-Russian classifier of products - Wikipedia [Obshcherossijskij klassifikator produkcii — Wikipedia]}},
url = {https://ru.wikipedia.org/?oldid=93314460},
urldate = {2018-08-31}
}
@misc{wiki-nigp,
title = {{NIGP Code - Wikipedia}},
url = {https://en.wikipedia.org/wiki/NIGP{\_}Code},
urldate = {2018-08-31}
}
@article{ruder-survey,
abstract = {Cross-lingual representations of words enable us to reason about word meaning in multilingual contexts and are a key facilitator of cross-lingual transfer when developing natural language processing models for low-resource languages. In this survey, we provide a comprehensive typology of cross-lingual word embedding models. We compare their data requirements and objective functions. The recurring theme of the survey is that many of the models presented in the literature optimize for the same objectives, and that seemingly different models are often equivalent modulo optimization strategies, hyper-parameters, and such. We also discuss the different ways cross-lingual word embeddings are evaluated, as well as future challenges and research horizons.},
archivePrefix = {arXiv},
arxivId = {1706.04902},
author = {Ruder, Sebastian and Vuli{\'{c}}, Ivan and S{\o}gaard, Anders},
eprint = {1706.04902},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ruder, Vuli{\'{c}}, S{\o}gaard - 2017 - A Survey Of Cross-lingual Word Embedding Models.pdf:pdf},
month = {jun},
title = {{A Survey Of Cross-lingual Word Embedding Models}},
url = {http://arxiv.org/abs/1706.04902},
year = {2017}
}
@article{muse,
abstract = {State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available.},
archivePrefix = {arXiv},
arxivId = {1710.04087},
author = {Conneau, Alexis and Lample, Guillaume and Ranzato, Marc'Aurelio and Denoyer, Ludovic and J{\'{e}}gou, Herv{\'{e}}},
eprint = {1710.04087},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Conneau et al. - 2017 - Word Translation Without Parallel Data.pdf:pdf},
month = {oct},
title = {{Word Translation Without Parallel Data}},
url = {http://arxiv.org/abs/1710.04087},
year = {2017}
}
@inproceedings{gensim,
address = {Valletta, Malta},
annote = {$\backslash$url{\{}http://is.muni.cz/publication/884893/en{\}}},
author = {Řehůřek, Radim and Sojka, Petr},
booktitle = {Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks},
keywords = {specom-17},
mendeley-tags = {specom-17},
month = {may},
pages = {45--50},
publisher = {ELRA},
title = {{Software Framework for Topic Modelling with Large Corpora}},
year = {2010}
}
@inproceedings{mikolov2013,
abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
archivePrefix = {arXiv},
arxivId = {1301.3781},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
booktitle = {Proceedings of the International Conference on Learning Representations (ICLR 2013)},
eprint = {1301.3781},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov et al. - Unknown - Vector Space.pdf:pdf},
keywords = {specom-17},
mendeley-tags = {specom-17},
month = {jan},
pages = {1--12},
title = {{Efficient estimation of word representations in vector space}},
url = {http://arxiv.org/abs/1301.3781},
year = {2013}
}
@article{doc2vec,
author = {Le, QV Quoc and Mikolov, Tomas and Com, Tmikolov Google},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Le, Mikolov, Com - 2014 - Distributed Representations of Sentences and Documents.pdf:pdf},
journal = {arXiv preprint arXiv:1405.4053},
title = {{Distributed representations of sentences and documents}},
url = {https://arxiv.org/abs/1405.4053},
volume = {32},
year = {2014}
}
@article{mikolov-representations-2013,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large num- ber of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alterna- tive to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of “Canada” and “Air” cannot be easily combined to obtain “Air Canada”. Motivated by this example,we present a simplemethod for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
archivePrefix = {arXiv},
arxivId = {1310.4546},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
doi = {10.1162/jmlr.2003.3.4-5.951},
eprint = {1310.4546},
issn = {10495258},
journal = {Nips},
keywords = {specom-17},
mendeley-tags = {specom-17},
pages = {3111--3119},
pmid = {903},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
year = {2013}
}
