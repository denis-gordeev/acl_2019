Automatically generated by Mendeley Desktop 1.19.2
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@inproceedings{sag2002multiword,
author = {Sag, Ivan A and Baldwin, Timothy and Bond, Francis and Copestake, Ann and Flickinger, Dan},
booktitle = {International Conference on Intelligent Text Processing and Computational Linguistics},
organization = {Springer},
pages = {1--15},
title = {{Multiword expressions: A pain in the neck for NLP}},
year = {2002}
}
@article{wordnet,
author = {Miller, George A.},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/ACM, 1995 - Unknown - WordNet a lexical database for English.pdf:pdf},
journal = {Communications of the ACM},
number = {11},
pages = {39--41},
title = {{WordNet: a lexical database for English}},
url = {https://dl.acm.org/citation.cfm?id=219748},
volume = {38},
year = {1995}
}
@inproceedings{mao-semeval,
address = {Stroudsburg, PA, USA},
author = {Mao, Rui and Chen, Guanyi and Li, Ruizhe and Lin, Chenghua},
booktitle = {Proceedings of The 12th International Workshop on Semantic Evaluation},
doi = {10.18653/v1/S18-1169},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mao et al. - 2018 - ABDN at SemEval-2018 Task 10 Recognising Discriminative Attributes using Context Embeddings and WordNet.pdf:pdf},
pages = {1017--1021},
publisher = {Association for Computational Linguistics},
title = {{ABDN at SemEval-2018 Task 10: Recognising Discriminative Attributes using Context Embeddings and WordNet}},
url = {http://aclweb.org/anthology/S18-1169},
year = {2018}
}
@inproceedings{gensim,
address = {Valletta, Malta},
annote = {$\backslash$url{\{}http://is.muni.cz/publication/884893/en{\}}},
author = {Řehůřek, Radim and Sojka, Petr},
booktitle = {Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks},
keywords = {specom-17},
mendeley-tags = {specom-17},
month = {may},
pages = {45--50},
publisher = {ELRA},
title = {{Software Framework for Topic Modelling with Large Corpora}},
year = {2010}
}
@article{30years-graphs,
abstract = {A recent paper posed the question: “Graph Matching: What are we really talking about?”. Far from providing a definite answer to that question, in this paper we will try to characterize the role that graphs play within the Pattern Recognition field. To this aim two taxonomies are presented and discussed. The first includes almost all the graph matching algorithms proposed from the late seventies, and describes the different classes of algorithms. The second taxonomy considers the types of common applications of graph-based techniques in the Pattern Recognition and Machine Vision field.},
author = {Conte, D. and Foggia, P. and Sansone, C. and Vento, M.},
doi = {10.1142/S0218001404003228},
journal = {International Journal of Pattern Recognition and Artificial Intelligence},
month = {may},
number = {03},
pages = {265--298},
title = {{Thirty Years Of Graph Matching In Pattern Recognition}},
url = {http://www.worldscientific.com/doi/abs/10.1142/S0218001404003228},
volume = {18},
year = {2004}
}
@article{boumal2014manopt,
author = {Boumal, Nicolas and Mishra, Bamdev and Absil, P-A and Sepulchre, Rodolphe},
journal = {The Journal of Machine Learning Research},
number = {1},
pages = {1455--1459},
publisher = {JMLR. org},
title = {{Manopt, a Matlab toolbox for optimization on manifolds}},
volume = {15},
year = {2014}
}
@article{hedging-bets,
author = {Deng, J and Krause, J and on {\ldots}, AC Berg - 2012 IEEE Conference and 2012, Undefined},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Deng et al. - Unknown - Hedging your bets Optimizing accuracy-specificity trade-offs in large scale visual recognition.pdf:pdf},
journal = {ieeexplore.ieee.org},
title = {{Hedging your bets: Optimizing accuracy-specificity trade-offs in large scale visual recognition}},
url = {https://ieeexplore.ieee.org/abstract/document/6248086/}
}
@misc{unsd,
title = {{UNSD — National Classifications}},
url = {https://unstats.un.org/unsd/classifications/nationalclassifications/},
urldate = {2018-09-11}
}
@article{ruder-muse-limitations,
abstract = {Unsupervised machine translation---i.e., not assuming any cross-lingual supervision signal, whether a dictionary, translations, or comparable corpora---seems impossible, but nevertheless, Lample et al. (2018) recently proposed a fully unsupervised machine translation (MT) model. The model relies heavily on an adversarial, unsupervised alignment of word embedding spaces for bilingual dictionary induction (Conneau et al., 2018), which we examine here. Our results identify the limitations of current unsupervised MT: unsupervised bilingual dictionary induction performs much worse on morphologically rich languages that are not dependent marking, when monolingual corpora from different domains or different embedding algorithms are used. We show that a simple trick, exploiting a weak supervision signal from identical words, enables more robust induction, and establish a near-perfect correlation between unsupervised bilingual dictionary induction performance and a previously unexplored graph similarity metric.},
archivePrefix = {arXiv},
arxivId = {1805.03620},
author = {S{\o}gaard, Anders and Ruder, Sebastian and Vuli{\'{c}}, Ivan},
eprint = {1805.03620},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/S{\o}gaard, Ruder, Vuli{\'{c}} - 2018 - On the Limitations of Unsupervised Bilingual Dictionary Induction.pdf:pdf},
month = {may},
title = {{On the Limitations of Unsupervised Bilingual Dictionary Induction}},
url = {http://arxiv.org/abs/1805.03620},
year = {2018}
}
@article{alsuhaibani,
author = {Alsuhaibani, Mohammed and Maehara, Takanori and Bollegala, Danushka},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Alsuhaibani, Maehara, Bollegala - 2018 - Joint Learning of Hierarchical Word Embeddings from a Corpus and a Taxonomy.pdf:pdf},
month = {nov},
pages = {1--19},
title = {{Joint Learning of Hierarchical Word Embeddings from a Corpus and a Taxonomy}},
url = {https://openreview.net/forum?id=S1xf-W5paX{\&}noteId=rkfQ2CtNrE},
year = {2019}
}
@article{umap,
abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
archivePrefix = {arXiv},
arxivId = {1802.03426},
author = {McInnes, Leland and Healy, John and Melville, James},
eprint = {1802.03426},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/McInnes, Healy, Melville - 2018 - UMAP Uniform Manifold Approximation and Projection for Dimension Reduction.pdf:pdf},
month = {feb},
title = {{UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction}},
url = {http://arxiv.org/abs/1802.03426},
year = {2018}
}
@inproceedings{lgbm,
abstract = {Gradient Boosting Decision Tree (GBDT) is a popular machine learning algo- rithm, and has quite a few effective implementations such as XGBoost and pGBRT. Although many engineering optimizations have been adopted in these implementations, the efficiency and scalability are still unsatisfactory when the feature dimension is high and data size is large. A major reason is that for each feature, they need to scan all the data instances to estimate the information gain of all possible split points, which is very time consuming. To tackle this problem, we propose two novel techniques: Gradient-based One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB). With GOSS, we exclude a significant proportion of data instances with small gradients, and only use the rest to estimate the information gain. We prove that, since the data instances with larger gradients play a more important role in the computation of information gain, GOSS can obtain quite accurate estimation of the information gain with a much smaller data size. With EFB, we bundle mutually exclusive features (i.e., they rarely take nonzero values simultaneously), to reduce the number of features. We prove that finding the optimal bundling of exclusive features is NP-hard, but a greedy algorithm can achieve quite good approximation ratio (and thus can effectively reduce the number of features without hurting the accuracy of split point determination by much). We call our new GBDT implementation with GOSS and EFB LightGBM. Our experiments on multiple public datasets show that, LightGBM speeds up the training process of conventional GBDT by up to over 20 times while achieving almost the same accuracy.},
author = {Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-yan},
booktitle = {Advances in Neural Information Processing Systems 30 (NIPS 2017)},
doi = {10.1145/1731903.1731925},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ke et al. - Unknown - Lightgbm A highly efficient gradient boosting decision tree.pdf:pdf},
isbn = {9781605587332},
issn = {10495258},
pages = {3149--3157},
title = {{LightGBM : A Highly Efficient Gradient Boosting Decision Tree}},
url = {http://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradi},
year = {2017}
}
@misc{gos-zakupki,
title = {{The official site of the Unified Information System in the field of procurement [Oficial'nyj sajt Edinoj informacionnoj sistemy v sfere zakupok]}},
url = {http://www.zakupki.gov.ru/},
urldate = {2018-09-04}
}
@article{laser,
abstract = {State-of-the-art natural language processing systems rely on supervision in the form of annotated data to learn competent models. These models are generally trained on data in a single language (usually English), and cannot be directly used beyond that language. Since collecting data in every language is not realistic, there has been a growing interest in cross-lingual language understanding (XLU) and low-resource cross-language transfer. In this work, we construct an evaluation set for XLU by extending the development and test sets of the Multi-Genre Natural Language Inference Corpus (MultiNLI) to 15 languages, including low-resource languages such as Swahili and Urdu. We hope that our dataset, dubbed XNLI, will catalyze research in cross-lingual sentence understanding by providing an informative standard evaluation task. In addition, we provide several baselines for multilingual sentence understanding, including two based on machine translation systems, and two that use parallel data to train aligned multilingual bag-of-words and LSTM encoders. We find that XNLI represents a practical and challenging evaluation suite, and that directly translating the test data yields the best performance among available baselines.},
archivePrefix = {arXiv},
arxivId = {1809.05053},
author = {Conneau, Alexis and Lample, Guillaume and Rinott, Ruty and Williams, Adina and Bowman, Samuel R. and Schwenk, Holger and Stoyanov, Veselin},
eprint = {1809.05053},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Conneau et al. - Unknown - Xnli Evaluating cross-lingual sentence representations.pdf:pdf},
journal = {arxiv.org},
title = {{XNLI: Evaluating Cross-lingual Sentence Representations}},
url = {https://arxiv.org/abs/1809.05053 http://arxiv.org/abs/1809.05053},
year = {2018}
}
@article{ontology-sota,
author = {Shvaiko, P and on knowledge And, J Euzenat - IEEE Transactions and 2013, Undefined},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shvaiko, and, 2013 - Unknown - Ontology matching state of the art and future challenges.pdf:pdf},
journal = {ieeexplore.ieee.org},
title = {{Ontology matching: state of the art and future challenges}},
url = {https://ieeexplore.ieee.org/abstract/document/6104044/}
}
@article{tsne,
author = {van der Maaten, Laurens and Hinton, Geoffrey},
journal = {Journal of machine learning research},
number = {Nov},
pages = {2579--2605},
title = {{Visualizing data using t-SNE}},
volume = {9},
year = {2008}
}
@inproceedings{tarouti,
address = {Stroudsburg, PA, USA},
author = {Al tarouti, Feras and Kalita, Jugal},
booktitle = {Proceedings of the Workshop on Multilingual and Cross­-lingual Methods in NLP},
doi = {10.18653/v1/W16-1204},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Al tarouti, Kalita - 2016 - Enhancing Automatic Wordnet Construction Using Word Embeddings.pdf:pdf},
pages = {30--34},
publisher = {Association for Computational Linguistics},
title = {{Enhancing Automatic Wordnet Construction Using Word Embeddings}},
url = {http://aclweb.org/anthology/W16-1204},
year = {2016}
}
@article{Joulin2016,
abstract = {This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore{\~{}}CPU, and classify half a million sentences among{\~{}}312K classes in less than a minute.},
archivePrefix = {arXiv},
arxivId = {1607.01759},
author = {Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},
eprint = {1607.01759},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Joulin et al. - 2016 - Bag of Tricks for Efficient Text Classification(2).pdf:pdf},
month = {jul},
title = {{Bag of Tricks for Efficient Text Classification}},
url = {http://arxiv.org/abs/1607.01759},
year = {2016}
}
@article{fasttext,
abstract = {This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore{\~{}}CPU, and classify half a million sentences among{\~{}}312K classes in less than a minute.},
archivePrefix = {arXiv},
arxivId = {1607.01759},
author = {Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},
eprint = {1607.01759},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Joulin et al. - 2016 - Bag of Tricks for Efficient Text Classification.pdf:pdf},
month = {jul},
title = {{Bag of Tricks for Efficient Text Classification}},
url = {http://arxiv.org/abs/1607.01759},
year = {2016}
}
@article{Arora2017,
abstract = {The success of neural network methods for computing word embeddings has mo-tivated methods for generating semantic embeddings of longer pieces of text, such as sentences and paragraphs. Surprisingly, Wieting et al (ICLR'16) showed that such complicated methods are outperformed, especially in out-of-domain (transfer learning) settings, by simpler methods involving mild retraining of word embed-dings and basic linear regression. The method of Wieting et al. requires retraining with a substantial labeled dataset such as Paraphrase Database (Ganitkevitch et al., 2013). The current paper goes further, showing that the following completely unsuper-vised sentence embedding is a formidable baseline: Use word embeddings com-puted using one of the popular methods on unlabeled corpus like Wikipedia, rep-resent the sentence by a weighted average of the word vectors, and then modify them a bit using PCA/SVD. This weighting improves performance by about 10{\%} to 30{\%} in textual similarity tasks, and beats sophisticated supervised methods in-cluding RNN's and LSTM's. It even improves Wieting et al.'s embeddings. This simple method should be used as the baseline to beat in future, especially when labeled training data is scarce or nonexistent. The paper also gives a theoretical explanation of the success of the above unsu-pervised method using a latent variable generative model for sentences, which is a simple extension of the model in Arora et al. (TACL'16) with new " smoothing " terms that allow for words occurring out of context, as well as high probabilities for words like and, not in all contexts.},
author = {Arora, Sanjeev and Liang, Yingyu and Ma, Tengyu},
journal = {ICLR},
pages = {1--14},
title = {{A Simple but Tough-to-Beat Baseline for Sentence Embeddings}},
url = {https://openreview.net/forum?id=SyK00v5xx},
year = {2017}
}
@book{lawler,
abstract = {Originally published: New York : Holt, Rinehart, and Winston, {\textcopyright}1976. Perceptively written text examines optimization problems that can be formulated in terms of networks and algebraic structures called matroids. Chapters cover shortest paths, network flows, bipartite matching, nonbipartite matching, matroids and the greedy algorithm, matroid intersections, and the matroid parity problems. A suitable text or reference for courses in combinatorial computing.},
author = {Lawler, Eugene L.},
isbn = {0486414531},
pages = {374},
title = {{Combinatorial Optimization : Networks And Matroids}},
url = {https://books.google.ru/books/about/Combinatorial{\_}Optimization.html?id=m4MvtFenVjEC{\&}redir{\_}esc=y}
}
@inproceedings{Swoboda2016,
address = {New York, New York, USA},
author = {Swoboda, Tobias and Hemmje, Matthias and Dascalu, Mihai and Trausan-Matu, Stefan},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering - DocEng '16},
doi = {10.1145/2960811.2967151},
isbn = {9781450344388},
keywords = {automated semantic integration,ontology alignment,taxonomy integration,word2vec},
pages = {131--134},
publisher = {ACM Press},
title = {{Combining Taxonomies using Word2vec}},
url = {http://dl.acm.org/citation.cfm?doid=2960811.2967151},
year = {2016}
}
@misc{data-gov,
title = {{Datasets - Data.gov}},
url = {https://catalog.data.gov/dataset},
urldate = {2018-09-04}
}
@misc{wiki-nigp,
title = {{NIGP Code - Wikipedia}},
url = {https://en.wikipedia.org/wiki/NIGP{\_}Code},
urldate = {2018-08-31}
}
@article{mikolov-parallel,
abstract = {Dictionaries and phrase tables are the basis of modern statistical machine translation systems. This paper develops a method that can automate the process of generating and extending dictionaries and phrase tables. Our method can translate missing word and phrase entries by learning language structures based on large monolingual data and mapping between languages from small bilingual data. It uses distributed representation of words and learns a linear mapping between vector spaces of languages. Despite its simplicity, our method is surprisingly effective: we can achieve almost 90{\%} precision@5 for translation of words between English and Spanish. This method makes little assumption about the languages, so it can be used to extend and refine dictionaries and translation tables for any language pairs.},
archivePrefix = {arXiv},
arxivId = {1309.4168},
author = {Mikolov, Tomas and Le, Quoc V. and Sutskever, Ilya},
eprint = {1309.4168},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov, Le, Sutskever - 2013 - Exploiting Similarities among Languages for Machine Translation.pdf:pdf},
month = {sep},
title = {{Exploiting Similarities among Languages for Machine Translation}},
url = {http://arxiv.org/abs/1309.4168},
year = {2013}
}
@misc{wiki-okpd,
title = {{All-Russian classifier of products - Wikipedia [Obshcherossijskij klassifikator produkcii — Wikipedia]}},
url = {https://ru.wikipedia.org/?oldid=93314460},
urldate = {2018-08-31}
}
@inproceedings{sand2017wordnet,
author = {Sand, Heidi and Velldal, Erik and {\O}vrelid, Lilja},
booktitle = {Proceedings of the 21st Nordic Conference on Computational Linguistics},
pages = {298--302},
title = {{Wordnet extension via word embeddings: Experiments on the Norwegian Wordnet}},
year = {2017}
}
@article{bouma2009normalized,
author = {Bouma, Gerlof},
journal = {Proceedings of GSCL},
pages = {31--40},
title = {{Normalized (pointwise) mutual information in collocation extraction}},
year = {2009}
}
@article{muse,
abstract = {State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available.},
archivePrefix = {arXiv},
arxivId = {1710.04087},
author = {Conneau, Alexis and Lample, Guillaume and Ranzato, Marc'Aurelio and Denoyer, Ludovic and J{\'{e}}gou, Herv{\'{e}}},
eprint = {1710.04087},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Conneau et al. - 2017 - Word Translation Without Parallel Data.pdf:pdf},
month = {oct},
title = {{Word Translation Without Parallel Data}},
url = {http://arxiv.org/abs/1710.04087},
year = {2017}
}
@inproceedings{mikolov2013,
abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
archivePrefix = {arXiv},
arxivId = {1301.3781},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
booktitle = {Proceedings of the International Conference on Learning Representations (ICLR 2013)},
eprint = {1301.3781},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov et al. - Unknown - Vector Space.pdf:pdf},
keywords = {specom-17},
mendeley-tags = {specom-17},
month = {jan},
pages = {1--12},
title = {{Efficient estimation of word representations in vector space}},
url = {http://arxiv.org/abs/1301.3781},
year = {2013}
}
@inproceedings{lazaridou-parallel,
address = {Stroudsburg, PA, USA},
author = {Lazaridou, Angeliki and Dinu, Georgiana and Baroni, Marco},
booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
doi = {10.3115/v1/P15-1027},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lazaridou, Dinu, Baroni - 2015 - Hubness and Pollution Delving into Cross-Space Mapping for Zero-Shot Learning.pdf:pdf},
pages = {270--280},
publisher = {Association for Computational Linguistics},
title = {{Hubness and Pollution: Delving into Cross-Space Mapping for Zero-Shot Learning}},
url = {http://aclweb.org/anthology/P15-1027},
volume = {1},
year = {2015}
}
@article{Bakarov2018,
abstract = {Word embeddings are real-valued word representations able to capture lexical semantics and trained on natural language corpora. Models proposing these representations have gained popularity in the recent years, but the issue of the most adequate evaluation method still remains open. This paper presents an extensive overview of the field of word embeddings evaluation, highlighting main problems and proposing a typology of approaches to evaluation, summarizing 16 intrinsic methods and 12 extrinsic methods. I describe both widely-used and experimental methods, systematize information about evaluation datasets and discuss some key challenges.},
archivePrefix = {arXiv},
arxivId = {1801.09536},
author = {Bakarov, Amir},
eprint = {1801.09536},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bakarov - 2018 - A Survey of Word Embeddings Evaluation Methods.pdf:pdf},
month = {jan},
title = {{A Survey of Word Embeddings Evaluation Methods}},
url = {http://arxiv.org/abs/1801.09536},
year = {2018}
}
@inproceedings{Khodak2017,
address = {Stroudsburg, PA, USA},
author = {Khodak, Mikhail and Risteski, Andrej and Fellbaum, Christiane and Arora, Sanjeev},
booktitle = {Proceedings of the 1st Workshop on Sense, Concept and Entity
          Representations and their Applications},
doi = {10.18653/v1/W17-1902},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Khodak et al. - 2017 - Automated WordNet Construction Using Word Embeddings.pdf:pdf},
pages = {12--23},
publisher = {Association for Computational Linguistics},
title = {{Automated WordNet Construction Using Word Embeddings}},
url = {http://aclweb.org/anthology/W17-1902},
year = {2017}
}
@book{dupe-detect,
address = {Berlin Heidelberg},
author = {Christen, Peter},
doi = {10.1007/978-3-642-31164-2},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Christen - 2012 - Data matching concepts and techniques for record linkage, entity resolution, and duplicate detection.pdf:pdf},
isbn = {978-3-642-31163-5},
pages = {272},
publisher = {Springer-Verlag},
title = {{Data Matching: Concepts and Techniques for Record Linkage, Entity Resolution, and Duplicate Detection}},
year = {2012}
}
@article{mikolov-representations-2013,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large num- ber of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alterna- tive to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of “Canada” and “Air” cannot be easily combined to obtain “Air Canada”. Motivated by this example,we present a simplemethod for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
archivePrefix = {arXiv},
arxivId = {1310.4546},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
doi = {10.1162/jmlr.2003.3.4-5.951},
eprint = {1310.4546},
issn = {10495258},
journal = {Nips},
keywords = {specom-17},
mendeley-tags = {specom-17},
pages = {3111--3119},
pmid = {903},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
year = {2013}
}
@article{glomo,
abstract = {Modern deep transfer learning approaches have mainly focused on learning generic feature vectors from one task that are transferable to other tasks, such as word embeddings in language and pretrained convolutional features in vision. However, these approaches usually transfer unary features and largely ignore more structured graphical representations. This work explores the possibility of learning generic latent relational graphs that capture dependencies between pairs of data units (e.g., words or pixels) from large-scale unlabeled data and transferring the graphs to downstream tasks. Our proposed transfer learning framework improves performance on various tasks including question answering, natural language inference, sentiment analysis, and image classification. We also show that the learned graphs are generic enough to be transferred to different embeddings on which the graphs have not been trained (including GloVe embeddings, ELMo embeddings, and task-specific RNN hidden unit), or embedding-free units such as image pixels.},
archivePrefix = {arXiv},
arxivId = {1806.05662},
author = {Yang, Zhilin and Zhao, Jake and Dhingra, Bhuwan and He, Kaiming and Cohen, William W. and Salakhutdinov, Ruslan and LeCun, Yann},
eprint = {1806.05662},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang et al. - 2018 - GLoMo Unsupervisedly Learned Relational Graphs as Transferable Representations.pdf:pdf},
month = {jun},
title = {{GLoMo: Unsupervisedly Learned Relational Graphs as Transferable Representations}},
url = {http://arxiv.org/abs/1806.05662},
year = {2018}
}
@article{google-translate-rare,
abstract = {Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units ("wordpieces") for both input and output. This method provides a good balance between the flexibility of "character"-delimited models and the efficiency of "word"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60{\%} compared to Google's phrase-based production system.},
archivePrefix = {arXiv},
arxivId = {1609.08144},
author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V.},
eprint = {1609.08144},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu et al. - 2016 - Google's Neural Machine Translation System Bridging the Gap between Human and Machine Translation.pdf:pdf},
month = {sep},
title = {{Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation}},
url = {http://arxiv.org/abs/1609.08144},
year = {2016}
}
@article{hungarian-listing,
author = {Brauner, N and Echahed, R and Finke, G and Gregor, H and Prost, F},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Brauner et al. - 2004 - A complete assignment algorithm and its application in constraint declarative languages.pdf:pdf},
journal = {Les cahiers du laboratoire Leibniz},
title = {{A complete assignment algorithm and its application in constraint declarative languages}},
url = {https://hal.archives-ouvertes.fr/hal-00082787/},
volume = {111},
year = {2004}
}
@inproceedings{sanchez2017well,
author = {Sanchez, Ivan and Riedel, Sebastian},
booktitle = {Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers},
pages = {401--407},
title = {{How Well Can We Predict Hypernyms from Word Embeddings? A Dataset-Centric Analysis}},
year = {2017}
}
@book{gorelick2014high,
author = {Gorelick, Micha and Ozsvald, Ian},
publisher = {" O'Reilly Media, Inc."},
title = {{High Performance Python: Practical Performant Programming for Humans}},
year = {2014}
}
@article{savary2018parseme,
author = {Savary, Agata and Candito, Marie and {Barbu Mititelu}, V and Bej{\v{c}}ek, Eduard and Cap, Fabienne and van Gompel, M},
publisher = {Berlin: Language Science Press},
title = {{PARSEME multilingual corpus of verbal multiword expressions}},
year = {2018}
}
@article{vecmap,
abstract = {Recent work has managed to learn cross-lingual word embeddings without parallel data by mapping monolingual embeddings to a shared space through adversarial training. However, their evaluation has focused on favorable conditions, using comparable corpora or closely-related languages, and we show that they often fail in more realistic scenarios. This work proposes an alternative approach based on a fully unsupervised initialization that explicitly exploits the structural similarity of the embeddings, and a robust self-learning algorithm that iteratively improves this solution. Our method succeeds in all tested scenarios and obtains the best published results in standard datasets, even surpassing previous supervised systems. Our implementation is released as an open source project at https://github.com/artetxem/vecmap},
archivePrefix = {arXiv},
arxivId = {1805.06297},
author = {Artetxe, Mikel and Labaka, Gorka and Agirre, Eneko},
eprint = {1805.06297},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Artetxe, Labaka, Agirre - 2018 - A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings.pdf:pdf},
month = {may},
title = {{A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings}},
url = {http://arxiv.org/abs/1805.06297},
year = {2018}
}
@inproceedings{dinu,
abstract = {The zero-shot paradigm exploits vector-based word representations extracted from text corpora with unsupervised methods to learn general mapping functions from other feature spaces onto word space, where the words associated to the nearest neighbours of the mapped vectors are used as their linguistic labels. We show that the neighbourhoods of the mapped elements are strongly polluted by hubs, vectors that tend to be near a high proportion of items, pushing their correct labels down the neighbour list. After illustrating the problem empirically, we propose a simple method to correct it by taking the proximity distribution of potential neighbours across many mapped vectors into account. We show that this correction leads to consistent improvements in realistic zero-shot experiments in the cross-lingual, image labeling and image retrieval domains.},
archivePrefix = {arXiv},
arxivId = {1412.6568},
author = {Dinu, Georgiana and Lazaridou, Angeliki and Baroni, Marco},
booktitle = {In Proceedings of the 3rd In- ternational Conference on Learning Representations (ICLR2015), workshop track},
eprint = {1412.6568},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dinu, Lazaridou, Baroni - 2014 - Improving zero-shot learning by mitigating the hubness problem.pdf:pdf},
month = {dec},
title = {{Improving zero-shot learning by mitigating the hubness problem}},
url = {http://arxiv.org/abs/1412.6568},
year = {2015}
}
@article{dropout,
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks from Overfitting.pdf:pdf},
journal = {Journal of Machine Learning Research},
pages = {1929--1958},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
url = {http://jmlr.org/papers/v15/srivastava14a.html},
volume = {15},
year = {2014}
}
@article{kutuzovgraphwordnet,
abstract = {We present path2vec, a new approach for learning graph embeddings that relies on structural measures of pairwise node similarities. The model learns representations for nodes in a dense space that approximate a given user-defined graph distance measure, such as e.g. the shortest path distance or distance measures that take information beyond the graph structure into account. Evaluation of the proposed model on semantic similarity and word sense disambiguation tasks, using various WordNet-based similarity measures, show that our approach yields competitive results, outperforming strong graph embedding baselines. The model is computationally efficient, being orders of magnitude faster than the direct computation of graph-based distances.},
archivePrefix = {arXiv},
arxivId = {1808.05611},
author = {Kutuzov, Andrey and Dorgham, Mohammad and Oliynyk, Oleksiy and Biemann, Chris and Panchenko, Alexander},
eprint = {1808.05611},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kutuzov et al. - 2018 - Learning Graph Embeddings from WordNet-based Similarity Measures.pdf:pdf},
month = {aug},
title = {{Learning Graph Embeddings from WordNet-based Similarity Measures}},
url = {http://arxiv.org/abs/1808.05611},
year = {2018}
}
@inproceedings{tufis2006romanian,
author = {Tufis, Dan and Mititelu, Verginica Barbu and Bozianu, Luigi and Mihaila, Catalin},
booktitle = {Proceedings of the 3rd Conference of the Global WordNet Association},
pages = {337--344},
title = {{Romanian wordnet: New developments and applications}},
year = {2006}
}
@article{jawanpuria,
abstract = {We propose a novel geometric approach for learning bilingual mappings given monolingual embeddings and a bilingual dictionary. Our approach decouples learning the transformation from the source language to the target language into (a) learning rotations for language-specific embeddings to align them to a common space, and (b) learning a similarity metric in the common space to model similarities between the embeddings. We model the bilingual mapping problem as an optimization problem on smooth Riemannian manifolds. We show that our approach outperforms previous approaches on the bilingual lexicon induction and cross-lingual word similarity tasks. We also generalize our framework to represent multiple languages in a common latent space. In particular, the latent space representations for several languages are learned jointly, given bilingual dictionaries for multiple language pairs. We illustrate the effectiveness of joint learning for multiple languages in zero-shot word translation setting.},
archivePrefix = {arXiv},
arxivId = {1808.08773},
author = {Jawanpuria, Pratik and Balgovind, Arjun and Kunchukuttan, Anoop and Mishra, Bamdev},
eprint = {1808.08773},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jawanpuria et al. - 2018 - Learning Multilingual Word Embeddings in Latent Metric Space A Geometric Approach.pdf:pdf},
month = {aug},
title = {{Learning Multilingual Word Embeddings in Latent Metric Space: A Geometric Approach}},
url = {http://arxiv.org/abs/1808.08773},
year = {2018}
}
@article{Liu,
author = {Liu, J and Chang, WC and Wu, Y and {\ldots}, Y Yang - of the 40th International ACM SIGIR and undefined 2017},
journal = {dl.acm.org},
title = {{Deep learning for extreme multi-label text classification}},
url = {https://dl.acm.org/citation.cfm?id=3080834}
}
@inproceedings{NEALE18.1030,
address = {Miyazaki, Japan},
author = {Neale, Steven},
booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)},
editor = {Chair), Nicoletta Calzolari (Conference and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Hasida, Koiti and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, H{\'{e}}l{\`{e}}ne and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios and Tokunaga, Takenobu},
isbn = {979-10-95546-00-9},
publisher = {European Language Resources Association (ELRA)},
title = {{A Survey on Automatically-Constructed WordNets and their Evaluation: Lexical and Word Embedding-based Approaches}},
year = {2018}
}
@inproceedings{joulin2018loss,
author = {Joulin, Armand and Bojanowski, Piotr and Mikolov, Tomas and J{\'{e}}gou, Herv{\'{e}} and Grave, Edouard},
booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
pages = {2979--2984},
title = {{Loss in translation: Learning bilingual word mapping with a retrieval criterion}},
year = {2018}
}
@article{mikolov-parallel,
abstract = {Dictionaries and phrase tables are the basis of modern statistical machine translation systems. This paper develops a method that can automate the process of generating and extending dictionaries and phrase tables. Our method can translate missing word and phrase entries by learning language structures based on large monolingual data and mapping between languages from small bilingual data. It uses distributed representation of words and learns a linear mapping between vector spaces of languages. Despite its simplicity, our method is surprisingly effective: we can achieve almost 90{\%} precision@5 for translation of words between English and Spanish. This method makes little assumption about the languages, so it can be used to extend and refine dictionaries and translation tables for any language pairs.},
archivePrefix = {arXiv},
arxivId = {1309.4168},
author = {Mikolov, Tomas and Le, Quoc V. and Sutskever, Ilya},
eprint = {1309.4168},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov, Le, Sutskever - 2013 - Exploiting Similarities among Languages for Machine Translation(2).pdf:pdf},
month = {sep},
title = {{Exploiting Similarities among Languages for Machine Translation}},
url = {http://arxiv.org/abs/1309.4168},
year = {2013}
}
@article{tax2vec,
abstract = {The use of background knowledge remains largely unexploited in many text classification tasks. In this work, we explore word taxonomies as means for constructing new semantic features, which may improve the performance and robustness of the learned classifiers. We propose tax2vec, a parallel algorithm for constructing taxonomy based features, and demonstrate its use on six short-text classification problems, including gender, age and personality type prediction, drug effectiveness and side effect prediction, and news topic prediction. The experimental results indicate that the interpretable features constructed using tax2vec can notably improve the performance of classifiers; the constructed features, in combination with fast, linear classifiers tested against strong baselines, such as hierarchical attention neural networks, achieved comparable or better classification results on short documents. Further, tax2vec can also serve for extraction of corpus-specific keywords. Finally, we investigated the semantic space of potential features where we observe a similarity with the well known Zipf's law.},
archivePrefix = {arXiv},
arxivId = {1902.00438},
author = {{\v{S}}krlj, Bla{\v{z}} and Martinc, Matej and Kralj, Jan and Lavra{\v{c}}, Nada and Pollak, Senja},
eprint = {1902.00438},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/{\v{S}}krlj et al. - 2019 - tax2vec Constructing Interpretable Features from Taxonomies for Short Text Classification.pdf:pdf},
month = {feb},
title = {{tax2vec: Constructing Interpretable Features from Taxonomies for Short Text Classification}},
url = {http://arxiv.org/abs/1902.00438},
year = {2019}
}
@article{ruder-survey,
abstract = {Cross-lingual representations of words enable us to reason about word meaning in multilingual contexts and are a key facilitator of cross-lingual transfer when developing natural language processing models for low-resource languages. In this survey, we provide a comprehensive typology of cross-lingual word embedding models. We compare their data requirements and objective functions. The recurring theme of the survey is that many of the models presented in the literature optimize for the same objectives, and that seemingly different models are often equivalent modulo optimization strategies, hyper-parameters, and such. We also discuss the different ways cross-lingual word embeddings are evaluated, as well as future challenges and research horizons.},
archivePrefix = {arXiv},
arxivId = {1706.04902},
author = {Ruder, Sebastian and Vuli{\'{c}}, Ivan and S{\o}gaard, Anders},
eprint = {1706.04902},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ruder, Vuli{\'{c}}, S{\o}gaard - 2017 - A Survey Of Cross-lingual Word Embedding Models.pdf:pdf},
month = {jun},
title = {{A Survey Of Cross-lingual Word Embedding Models}},
url = {http://arxiv.org/abs/1706.04902},
year = {2017}
}
@book{Vossen1998,
abstract = {This book describes the main objective of EuroWordNet, which is the building of a multilingual database with lexical semantic networks or wordnets for several European languages. Each wordnet in the database represents a language-specific structure due to the unique lexicalization of concepts in languages. The concepts are inter-linked via a separate Inter-Lingual-Index, where equivalent concepts across languages should share the same index item. The flexible multilingual design of the database makes it possible to compare the lexicalizations and semantic structures, revealing answers to fundamental linguistic and philosophical questions which could never be answered before. How consistent are lexical semantic networks across languages, what are the language-specific differences of these networks, is there a language-universal ontology, how much information can be shared across languages? First attempts to answer these questions are given in the form of a set of shared or common Base Concepts that has been derived from the separate wordnets and their classification by a language-neutral top-ontology. These Base Concepts play a fundamental role in several wordnets. Nevertheless, the database may also serve many practical needs with respect to (cross-language) information retrieval, machine translation tools, language generation tools and language learning tools, which are discussed in the final chapter. The book offers an excellent introduction to the EuroWordNet project for scholars in the field and raises many issues that set the directions for further research in semantics and knowledge engineering.},
author = {Vossen, Piek.},
booktitle = {EuroWordNet: A multilingual database with lexical semantic networks},
doi = {10.1007/978-94-017-1491-4},
isbn = {0792352955},
pages = {179},
publisher = {Kluwer Academic},
title = {{EuroWordNet: A multilingual database with lexical semantic networks}},
year = {2013}
}
@book{Riesen2010,
author = {Riesen, Kaspar and Bunke, Horst},
doi = {10.1142/7731},
isbn = {978-981-4304-71-9},
month = {apr},
publisher = {World Scientific},
series = {Series in Machine Perception and Artificial Intelligence},
title = {{Graph Classification and Clustering Based on Vector Space Embedding}},
url = {https://www.worldscientific.com/worldscibooks/10.1142/7731},
volume = {77},
year = {2010}
}
@inproceedings{gordeev-fruct,
address = {Bologna},
author = {Gordeev, D and Rey, A and Shagarov, D},
booktitle = {Proceedings of the FRUCT'23},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gordeev, Rey, Shagarov - 2018 - Unsupervised Cross-lingual Matching of Product.pdf:pdf},
pages = {459--464},
publisher = {FRUCT Oy, Finland},
title = {{Unsupervised Cross-lingual Matching of Product}},
url = {https://www.fruct.org/publications/abstract23/files/Gor.pdf},
year = {2018}
}
@article{natekin2013gradient,
author = {Natekin, Alexey and Knoll, Alois},
journal = {Frontiers in neurorobotics},
pages = {21},
publisher = {Frontiers},
title = {{Gradient boosting machines, a tutorial}},
volume = {7},
year = {2013}
}
@article{doc2vec,
author = {Le, QV Quoc and Mikolov, Tomas and Com, Tmikolov Google},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Le, Mikolov, Com - 2014 - Distributed Representations of Sentences and Documents.pdf:pdf},
journal = {arXiv preprint arXiv:1405.4053},
title = {{Distributed representations of sentences and documents}},
url = {https://arxiv.org/abs/1405.4053},
volume = {32},
year = {2014}
}
@inproceedings{maslova-potapov,
author = {Maslova, Natalia and Potapov, Vsevolod},
booktitle = {Lecture Notes in Computer Science},
doi = {10.1007/978-3-319-66429-3_54},
isbn = {9783319664286},
issn = {16113349},
keywords = {Automated sentiment analysis,Deprivation,Russian,Social network discourse,Supervised learning,Word embeddings},
pages = {546--554},
title = {{Neural network doc2vec in automated sentiment analysis for short informal texts}},
url = {http://link.springer.com/10.1007/978-3-319-66429-3{\_}54},
volume = {10458 LNAI},
year = {2017}
}
@article{bengio,
author = {Bengio, Y and Ducharme, R and Vincent, P},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bengio et al. - Unknown - A neural probabilistic language model.pdf:pdf},
issn = {ISSN 1533-7928},
journal = {Journal of Machine Learning Research},
keywords = {artificial neural networks,curse of dimensionality,distributed representation,statistical language modeling},
pages = {1137--1155},
title = {{A neural probabilistic language model}},
url = {http://www.jmlr.org/papers/v3/bengio03a.html},
volume = {3},
year = {2003}
}
@article{sagot,
abstract = {This paper describes automatic construction a freely-available wordnet for French (WOLF) based on Princeton WordNet (PWN) by using various multilingual resources. Polysemous words were dealt with an approach in which a parallel corpus for five languages was word-aligned and the extracted multilingual lexicon was disambiguated with the existing wordnets for these languages. On the other hand, a bilingual approach sufficed to acquire equivalents for monosemous words. Bilingual lexicons were extracted from Wikipedia and thesauri. The results obtained from each resource were merged and ranked according to the number of resources yielding the same literal. Automatic evaluation of the merged wordnet was performed with the French WordNet (FREWN). Manual evaluation was also carried out on a sample of the generated synsets. Precision shows that the presented approach has proved to be very promising and applications to use the created wordnet are already intended.},
author = {Sagot, Beno{\^{i}}t and Fi{\v{s}}er, Darja},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sagot, Fi{\v{s}}er - 2008 - Building a free French wordnet from multilingual resources.pdf:pdf},
month = {may},
title = {{Building a free French wordnet from multilingual resources}},
url = {https://hal.inria.fr/inria-00614708/},
year = {2008}
}
@inproceedings{nltk,
author = {Bird, Steven},
booktitle = {Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions},
doi = {10.3115/1225403.1225421},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bird - 2006 - NLTK the natural language toolkit.pdf:pdf},
pages = {69--72},
publisher = {Association for Computational Linguistics},
title = {{NLTK: the natural language toolkit}},
year = {2006}
}
@inproceedings{bond-wordnet,
abstract = {We create an open multilingual wordnet with large wordnets for over 26 languages and smaller ones for 57 languages. It is made by combining wordnets with open li-cences, data from Wiktionary and the Uni-code Common Locale Data Repository. Overall there are over 2 million senses for over 100 thousand concepts, linking over 1.4 million words in hundreds of lan-guages.},
author = {Bond, Francis and Foster, Ryan},
booktitle = {Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL)},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bond, Foster - 2013 - Linking and Extending an Open Multilingual Wordnet.pdf:pdf},
pages = {1352--1362},
title = {{Linking and Extending an Open Multilingual Wordnet}},
url = {https://aclweb.org/anthology/papers/P/P13/P13-1133/ http://www.aclweb.org/anthology/P13-1133},
year = {2013}
}
@article{levy-goldberg-2015,
abstract = {Recent trends suggest that neural-network-inspired word embedding models outperform traditional count-based distri-butional models on word similarity and analogy detection tasks. We reveal that much of the performance gains of word embeddings are due to certain system design choices and hyperparameter op-timizations, rather than the embedding algorithms themselves. Furthermore, we show that these modifications can be transferred to traditional distributional models, yielding similar gains. In contrast to prior reports, we observe mostly local or insignificant performance differences between the methods, with no global advantage to any single approach over the others.},
archivePrefix = {arXiv},
arxivId = {1103.0398},
author = {Levy, Omer and Goldberg, Yoav and Dagan, Ido},
doi = {10.1186/1472-6947-15-S2-S2},
eprint = {1103.0398},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Levy et al. - Unknown - Improving distributional similarity with lessons learned from word embeddings.pdf:pdf},
issn = {2307-387X},
journal = {Transactions of the Association for Computational Linguistics},
pages = {211--225},
pmid = {26099735},
title = {{Improving Distributional Similarity with Lessons Learned from Word Embeddings}},
url = {https://www.transacl.org/ojs/index.php/tacl/article/view/570},
volume = {3},
year = {2015}
}
@article{bilda,
abstract = {In this paper, we study different applications of cross-language latent topic models trained on comparable corpora. The first focus lies on the task of cross-language information retrieval (CLIR). The Bilingual Latent Dirichlet allocation model (BiLDA) allows us to create an interlingual, language-independent representation of both queries and documents. We construct several BiLDA-based document models for CLIR, where no additional translation resources are used. The second focus lies on the methods for extracting translation candidates and semantically related words using only per-topic word distributions of the cross-language latent topic model. As the main contribution, we combine the two former steps, blending the evidences from the per-document topic dis- tributions and the per-topic word distributions of the topic model with the knowledge from the extracted lexicon. We design and evaluate the novel evidence-rich statistical model for CLIR, and prove that such a model, which combines various (only internal) evidences, obtains the best scores for experiments performed on the standard test collections of the CLEF 2001–2003 campaigns. We confirm these findings in an alternative evaluation, where we automatically generate queries and perform the known-item search on a test subset of Wikipedia articles. The main importance of this work lies in the fact that we train translation resources from comparable document-aligned corpora and provide novel CLIR statistical models that exhaustively exploit as many cross-lingual clues as possible in the quest for better CLIR results, without use of any additional external resources such as parallel corpora or machine-readable dictionaries.},
author = {Vuli{\'{c}}, Ivan and de Smet, Wim and Moens, Marie Francine},
doi = {10.1007/s10791-012-9200-5},
file = {:home/denis/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vuli{\'{c}}, De Smet, Moens - 2013 - Cross-language information retrieval models based on latent topic models trained with document-aligned c.pdf:pdf},
isbn = {1079101292},
issn = {13864564},
journal = {Information Retrieval},
keywords = {Cross-language information retrieval,Evidence-rich retrieval models,Probabilistic latent topic models,Unsupervised cross-language lexicon extraction},
month = {jun},
number = {3},
pages = {331--368},
publisher = {Springer Netherlands},
title = {{Cross-language information retrieval models based on latent topic models trained with document-aligned comparable corpora}},
url = {http://link.springer.com/10.1007/s10791-012-9200-5},
volume = {16},
year = {2013}
}
@inproceedings{short-lda,
abstract = {Nowadays, short texts are very prevalent in various web applications, such as microblogs, instant messages. The severe sparsity of short texts hinders existing topic models to learn reliable topics. In this paper, we propose a novel way to tackle this problem. The key idea is to learn topics by exploring term correlation data, rather than the high-dimensional and sparse term occurrence information in documents. Such term correlation data is less sparse and more stable with the increase of the collection size, and can well capture the necessary information for topic learning. To obtain reliable topics from term correlation data, we first introduce a novel way to compute term correlation in short texts by representing each term with its co-occurred terms. Then we formulated the topic learning problem as symmetric non-negative matrix factorization on the term correlation matrix. After learning the topics, we can easily infer the topics of documents. Experimental results on three data sets show that our method provides substantially better performance than the baseline methods.},
address = {Philadelphia, PA},
author = {Yan, Xiaohui and Guo, Jiafeng and Liu, Shenghua and Cheng, Xueqi and Wang, Yanfeng},
booktitle = {Proceedings of the 2013 SIAM International Conference on Data Mining},
doi = {10.1137/1.9781611972832.83},
isbn = {9781611972627},
issn = {9781611972627},
month = {may},
pages = {749--757},
publisher = {Society for Industrial and Applied Mathematics},
title = {{Learning Topics in Short Texts by Non-negative Matrix Factorization on Term Correlation Matrix}},
url = {http://epubs.siam.org/doi/abs/10.1137/1.9781611972832.83},
year = {2013}
}
@inproceedings{artetxe2016learning,
author = {Artetxe, Mikel and Labaka, Gorka and Agirre, Eneko},
booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
pages = {2289--2294},
title = {{Learning principled bilingual mappings of word embeddings while preserving monolingual invariance}},
year = {2016}
}
@inproceedings{faria,
author = {Faria, Daniel and Pesquita, Catia and Santos, Emanuel and Palmonari, Matteo and Cruz, Isabel F. and Couto, Francisco M.},
doi = {10.1007/978-3-642-41030-7_38},
pages = {527--541},
title = {{The AgreementMakerLight Ontology Matching System}},
url = {http://link.springer.com/10.1007/978-3-642-41030-7{\_}38},
year = {2013}
}
